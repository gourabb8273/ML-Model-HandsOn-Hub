{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classifier - Training of word2vec and average word2vec from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Label Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "messages = pd.read_csv('./Data/SMSSpamCollection.txt',sep='\\t',names=[\"labels\",\"message\"])\n",
    "msg = pd.read_csv('./Data/SMSSpamCollection.txt',sep='\\t',names=[\"labels\",\"message\"]) #will use for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels                                            message\n",
       "0       ham  Go until jurong point, crazy.. Available only ...\n",
       "1       ham                      Ok lar... Joking wif u oni...\n",
       "2      spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       ham  U dun say so early hor... U c already then say...\n",
       "4       ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...     ...                                                ...\n",
       "5567   spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568    ham               Will ü b going to esplanade fr home?\n",
       "5569    ham  Pity, * was in mood for that. So...any other s...\n",
       "5570    ham  The guy did some bitching but I acted like i'd...\n",
       "5571    ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning and Preprocessing\n",
    "- Using regex to clean special char\n",
    "- Using stopwords to clean insignificant words\n",
    "- Lowering the case\n",
    "- Stemming to reduce vocabulary and converting to root base word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) \n",
      "------------\n",
      ":-) :-)\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(messages )):\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', messages['message'][i]) # removing special char other than a to z\n",
    "    review = review.lower()  # lowering case\n",
    "    review = review.split()  # Getting all words as list from sentences or document\n",
    "    # stemming and reducing stop words adding stop word can reduce the entire sentence so very careful\n",
    "    # review = [ps.stem(word) for word in review if not word in stopwords.words('english')] \n",
    "    review = [ps.stem(word) for word in review] \n",
    "    review = ' '.join(review)\n",
    "    if(review): # if review is valid non empty then only add\n",
    "        corpus.append(review)\n",
    "    else:\n",
    "        print(messages['message'][i])        \n",
    "        messages.drop(i, inplace=True) # if any emogi or any unnecessary char present then drop it        \n",
    "        print(\"------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazi avail onli in bugi n great world la e buffet cine there got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri in 2 a wkli comp to win fa cup final tkt 21st may 2005 text fa to 87121 to receiv entri question std txt rate t c s appli 08452810075over18 s',\n",
       " 'u dun say so earli hor u c alreadi then say',\n",
       " 'nah i don t think he goe to usf he live around here though',\n",
       " 'freemsg hey there darl it s been 3 week s now and no word back i d like some fun you up for it still tb ok xxx std chg to send 1 50 to rcv',\n",
       " 'even my brother is not like to speak with me they treat me like aid patent',\n",
       " 'as per your request mell mell oru minnaminungint nurungu vettam ha been set as your callertun for all caller press 9 to copi your friend callertun',\n",
       " 'winner as a valu network custom you have been select to receivea 900 prize reward to claim call 09061701461 claim code kl341 valid 12 hour onli',\n",
       " 'had your mobil 11 month or more u r entitl to updat to the latest colour mobil with camera for free call the mobil updat co free on 08002986030',\n",
       " 'i m gonna be home soon and i don t want to talk about thi stuff anymor tonight k i ve cri enough today',\n",
       " 'six chanc to win cash from 100 to 20 000 pound txt csh11 and send to 87575 cost 150p day 6day 16 tsandc appli repli hl 4 info',\n",
       " 'urgent you have won a 1 week free membership in our 100 000 prize jackpot txt the word claim to no 81010 t c www dbuk net lccltd pobox 4403ldnw1a7rw18',\n",
       " 'i ve been search for the right word to thank you for thi breather i promis i wont take your help for grant and will fulfil my promis you have been wonder and a bless at all time',\n",
       " 'i have a date on sunday with will',\n",
       " 'xxxmobilemovieclub to use your credit click the wap link in the next txt messag or click here http wap xxxmobilemovieclub com n qjkgighjjgcbl',\n",
       " 'oh k i m watch here',\n",
       " 'eh u rememb how 2 spell hi name ye i did he v naughti make until i v wet',\n",
       " 'fine if that s the way u feel that s the way it gota b',\n",
       " 'england v macedonia dont miss the goal team news txt ur nation team to 87077 eg england to 87077 tri wale scotland 4txt 1 20 poboxox36504w45wq 16']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:20] #checking top 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any empty list present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sent for sent in corpus if sent] # removing empty list if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500,binary=True, ngram_range=(2,2)) # taking max 2500 occuring feature\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5570, 2500)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. One hot Label encoding for y feature like target like spam or Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5570, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=pd.get_dummies(messages['labels'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64),\n",
       " array([False, False, False, ...,  True, False, False]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train the ML using Multinomial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prediction and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712746858168761\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.97      0.98       974\n",
      "        True       0.84      0.95      0.89       140\n",
      "\n",
      "    accuracy                           0.97      1114\n",
      "   macro avg       0.92      0.96      0.94      1114\n",
      "weighted avg       0.97      0.97      0.97      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Using TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=2500, ngram_range=(1,2))\n",
    "X = tv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred=spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9820466786355476\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99       976\n",
      "        True       0.87      1.00      0.93       138\n",
      "\n",
      "    accuracy                           0.98      1114\n",
      "   macro avg       0.94      0.99      0.96      1114\n",
      "weighted avg       0.98      0.98      0.98      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF accuracy improved from 0.97 to 0.98 % we can also use other classifer like random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829443447037702"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99       975\n",
      "        True       0.88      1.00      0.94       139\n",
      "\n",
      "    accuracy                           0.98      1114\n",
      "   macro avg       0.94      0.99      0.96      1114\n",
      "weighted avg       0.98      0.98      0.98      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Word2vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5570\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use gensim either for using pre trained model or train a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use pretrained model but we will create from scratch\n",
    "\n",
    "# import gensim.downloader as api\n",
    "\n",
    "# wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 Using lemmatizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n",
      ":) \n",
      ":-) :-)\n"
     ]
    }
   ],
   "source": [
    "corpus_lem = []\n",
    "for i in range(0, len(msg)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', msg['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()    \n",
    "    # review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    review = ' '.join(review)\n",
    "    if(review):\n",
    "        corpus_lem.append(review)\n",
    "    else:\n",
    "        print(msg['message'][i])\n",
    "        msg.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazi avail onli in bugi n great world la e buffet cine there got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri in 2 a wkli comp to win fa cup final tkt 21st may 2005 text fa to 87121 to receiv entri question std txt rate t c s appli 08452810075over18 s',\n",
       " 'u dun say so earli hor u c alreadi then say',\n",
       " 'nah i don t think he goe to usf he live around here though',\n",
       " 'freemsg hey there darl it s been 3 week s now and no word back i d like some fun you up for it still tb ok xxx std chg to send 1 50 to rcv',\n",
       " 'even my brother is not like to speak with me they treat me like aid patent',\n",
       " 'as per your request mell mell oru minnaminungint nurungu vettam ha been set as your callertun for all caller press 9 to copi your friend callertun',\n",
       " 'winner as a valu network custom you have been select to receivea 900 prize reward to claim call 09061701461 claim code kl341 valid 12 hour onli',\n",
       " 'had your mobil 11 month or more u r entitl to updat to the latest colour mobil with camera for free call the mobil updat co free on 08002986030',\n",
       " 'i m gonna be home soon and i don t want to talk about thi stuff anymor tonight k i ve cri enough today',\n",
       " 'six chanc to win cash from 100 to 20 000 pound txt csh11 and send to 87575 cost 150p day 6day 16 tsandc appli repli hl 4 info',\n",
       " 'urgent you have won a 1 week free membership in our 100 000 prize jackpot txt the word claim to no 81010 t c www dbuk net lccltd pobox 4403ldnw1a7rw18',\n",
       " 'i ve been search for the right word to thank you for thi breather i promis i wont take your help for grant and will fulfil my promis you have been wonder and a bless at all time',\n",
       " 'i have a date on sunday with will',\n",
       " 'xxxmobilemovieclub to use your credit click the wap link in the next txt messag or click here http wap xxxmobilemovieclub com n qjkgighjjgcbl',\n",
       " 'oh k i m watch here',\n",
       " 'eh u rememb how 2 spell hi name ye i did he v naughti make until i v wet',\n",
       " 'fine if that s the way u feel that s the way it gota b',\n",
       " 'england v macedonia dont miss the goal team news txt ur nation team to 87077 eg england to 87077 tri wale scotland 4txt 1 20 poboxox36504w45wq 16']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go until jurong point crazi avail onli in bugi n great world la e buffet cine there got amor wat'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Lowering and tokenizing the sentences from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for index,sent in enumerate(corpus): \n",
    "    sent_token=sent_tokenize(sent)\n",
    "    if sent_token:\n",
    "        for sentence_token  in sent_token:\n",
    "            words.append(simple_preprocess(sentence_token ))  # lowering each words\n",
    "    else:\n",
    "        print(\"sent_token\",index, corpus[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All unique words of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go',\n",
       "  'until',\n",
       "  'jurong',\n",
       "  'point',\n",
       "  'crazi',\n",
       "  'avail',\n",
       "  'onli',\n",
       "  'in',\n",
       "  'bugi',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'buffet',\n",
       "  'cine',\n",
       "  'there',\n",
       "  'got',\n",
       "  'amor',\n",
       "  'wat'],\n",
       " ['ok', 'lar', 'joke', 'wif', 'oni'],\n",
       " ['free',\n",
       "  'entri',\n",
       "  'in',\n",
       "  'wkli',\n",
       "  'comp',\n",
       "  'to',\n",
       "  'win',\n",
       "  'fa',\n",
       "  'cup',\n",
       "  'final',\n",
       "  'tkt',\n",
       "  'st',\n",
       "  'may',\n",
       "  'text',\n",
       "  'fa',\n",
       "  'to',\n",
       "  'to',\n",
       "  'receiv',\n",
       "  'entri',\n",
       "  'question',\n",
       "  'std',\n",
       "  'txt',\n",
       "  'rate',\n",
       "  'appli',\n",
       "  'over'],\n",
       " ['dun', 'say', 'so', 'earli', 'hor', 'alreadi', 'then', 'say'],\n",
       " ['nah',\n",
       "  'don',\n",
       "  'think',\n",
       "  'he',\n",
       "  'goe',\n",
       "  'to',\n",
       "  'usf',\n",
       "  'he',\n",
       "  'live',\n",
       "  'around',\n",
       "  'here',\n",
       "  'though'],\n",
       " ['freemsg',\n",
       "  'hey',\n",
       "  'there',\n",
       "  'darl',\n",
       "  'it',\n",
       "  'been',\n",
       "  'week',\n",
       "  'now',\n",
       "  'and',\n",
       "  'no',\n",
       "  'word',\n",
       "  'back',\n",
       "  'like',\n",
       "  'some',\n",
       "  'fun',\n",
       "  'you',\n",
       "  'up',\n",
       "  'for',\n",
       "  'it',\n",
       "  'still',\n",
       "  'tb',\n",
       "  'ok',\n",
       "  'xxx',\n",
       "  'std',\n",
       "  'chg',\n",
       "  'to',\n",
       "  'send',\n",
       "  'to',\n",
       "  'rcv'],\n",
       " ['even',\n",
       "  'my',\n",
       "  'brother',\n",
       "  'is',\n",
       "  'not',\n",
       "  'like',\n",
       "  'to',\n",
       "  'speak',\n",
       "  'with',\n",
       "  'me',\n",
       "  'they',\n",
       "  'treat',\n",
       "  'me',\n",
       "  'like',\n",
       "  'aid',\n",
       "  'patent'],\n",
       " ['as',\n",
       "  'per',\n",
       "  'your',\n",
       "  'request',\n",
       "  'mell',\n",
       "  'mell',\n",
       "  'oru',\n",
       "  'minnaminungint',\n",
       "  'nurungu',\n",
       "  'vettam',\n",
       "  'ha',\n",
       "  'been',\n",
       "  'set',\n",
       "  'as',\n",
       "  'your',\n",
       "  'callertun',\n",
       "  'for',\n",
       "  'all',\n",
       "  'caller',\n",
       "  'press',\n",
       "  'to',\n",
       "  'copi',\n",
       "  'your',\n",
       "  'friend',\n",
       "  'callertun'],\n",
       " ['winner',\n",
       "  'as',\n",
       "  'valu',\n",
       "  'network',\n",
       "  'custom',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'select',\n",
       "  'to',\n",
       "  'receivea',\n",
       "  'prize',\n",
       "  'reward',\n",
       "  'to',\n",
       "  'claim',\n",
       "  'call',\n",
       "  'claim',\n",
       "  'code',\n",
       "  'kl',\n",
       "  'valid',\n",
       "  'hour',\n",
       "  'onli'],\n",
       " ['had',\n",
       "  'your',\n",
       "  'mobil',\n",
       "  'month',\n",
       "  'or',\n",
       "  'more',\n",
       "  'entitl',\n",
       "  'to',\n",
       "  'updat',\n",
       "  'to',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'colour',\n",
       "  'mobil',\n",
       "  'with',\n",
       "  'camera',\n",
       "  'for',\n",
       "  'free',\n",
       "  'call',\n",
       "  'the',\n",
       "  'mobil',\n",
       "  'updat',\n",
       "  'co',\n",
       "  'free',\n",
       "  'on'],\n",
       " ['gonna',\n",
       "  'be',\n",
       "  'home',\n",
       "  'soon',\n",
       "  'and',\n",
       "  'don',\n",
       "  'want',\n",
       "  'to',\n",
       "  'talk',\n",
       "  'about',\n",
       "  'thi',\n",
       "  'stuff',\n",
       "  'anymor',\n",
       "  'tonight',\n",
       "  've',\n",
       "  'cri',\n",
       "  'enough',\n",
       "  'today'],\n",
       " ['six',\n",
       "  'chanc',\n",
       "  'to',\n",
       "  'win',\n",
       "  'cash',\n",
       "  'from',\n",
       "  'to',\n",
       "  'pound',\n",
       "  'txt',\n",
       "  'csh',\n",
       "  'and',\n",
       "  'send',\n",
       "  'to',\n",
       "  'cost',\n",
       "  'day',\n",
       "  'day',\n",
       "  'tsandc',\n",
       "  'appli',\n",
       "  'repli',\n",
       "  'hl',\n",
       "  'info'],\n",
       " ['urgent',\n",
       "  'you',\n",
       "  'have',\n",
       "  'won',\n",
       "  'week',\n",
       "  'free',\n",
       "  'membership',\n",
       "  'in',\n",
       "  'our',\n",
       "  'prize',\n",
       "  'jackpot',\n",
       "  'txt',\n",
       "  'the',\n",
       "  'word',\n",
       "  'claim',\n",
       "  'to',\n",
       "  'no',\n",
       "  'www',\n",
       "  'dbuk',\n",
       "  'net',\n",
       "  'lccltd',\n",
       "  'pobox',\n",
       "  'ldnw',\n",
       "  'rw'],\n",
       " ['ve',\n",
       "  'been',\n",
       "  'search',\n",
       "  'for',\n",
       "  'the',\n",
       "  'right',\n",
       "  'word',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'thi',\n",
       "  'breather',\n",
       "  'promis',\n",
       "  'wont',\n",
       "  'take',\n",
       "  'your',\n",
       "  'help',\n",
       "  'for',\n",
       "  'grant',\n",
       "  'and',\n",
       "  'will',\n",
       "  'fulfil',\n",
       "  'my',\n",
       "  'promis',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'wonder',\n",
       "  'and',\n",
       "  'bless',\n",
       "  'at',\n",
       "  'all',\n",
       "  'time'],\n",
       " ['have', 'date', 'on', 'sunday', 'with', 'will'],\n",
       " ['to',\n",
       "  'use',\n",
       "  'your',\n",
       "  'credit',\n",
       "  'click',\n",
       "  'the',\n",
       "  'wap',\n",
       "  'link',\n",
       "  'in',\n",
       "  'the',\n",
       "  'next',\n",
       "  'txt',\n",
       "  'messag',\n",
       "  'or',\n",
       "  'click',\n",
       "  'here',\n",
       "  'http',\n",
       "  'wap',\n",
       "  'com',\n",
       "  'qjkgighjjgcbl'],\n",
       " ['oh', 'watch', 'here'],\n",
       " ['eh',\n",
       "  'rememb',\n",
       "  'how',\n",
       "  'spell',\n",
       "  'hi',\n",
       "  'name',\n",
       "  'ye',\n",
       "  'did',\n",
       "  'he',\n",
       "  'naughti',\n",
       "  'make',\n",
       "  'until',\n",
       "  'wet'],\n",
       " ['fine',\n",
       "  'if',\n",
       "  'that',\n",
       "  'the',\n",
       "  'way',\n",
       "  'feel',\n",
       "  'that',\n",
       "  'the',\n",
       "  'way',\n",
       "  'it',\n",
       "  'gota'],\n",
       " ['england',\n",
       "  'macedonia',\n",
       "  'dont',\n",
       "  'miss',\n",
       "  'the',\n",
       "  'goal',\n",
       "  'team',\n",
       "  'news',\n",
       "  'txt',\n",
       "  'ur',\n",
       "  'nation',\n",
       "  'team',\n",
       "  'to',\n",
       "  'eg',\n",
       "  'england',\n",
       "  'to',\n",
       "  'tri',\n",
       "  'wale',\n",
       "  'scotland',\n",
       "  'txt',\n",
       "  'poboxox',\n",
       "  'wq']]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Train word2vec from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create the feature and output based on window size and train the model to represent each word as vector dimension\n",
    "model =gensim.models.Word2Vec(words,window=5,min_count=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'you',\n",
       " 'the',\n",
       " 'it',\n",
       " 'and',\n",
       " 'in',\n",
       " 'is',\n",
       " 'me',\n",
       " 'my',\n",
       " 'for',\n",
       " 'your',\n",
       " 'call',\n",
       " 'that',\n",
       " 'of',\n",
       " 'have',\n",
       " 'on',\n",
       " 'now',\n",
       " 'do',\n",
       " 'are',\n",
       " 'can']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[:20] # all vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count #Total vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guarante', 0.9986960887908936),\n",
       " ('claim', 0.9986200928688049),\n",
       " ('cash', 0.9973024725914001),\n",
       " ('won', 0.9966235160827637),\n",
       " ('call', 0.9964011311531067),\n",
       " ('award', 0.9962254166603088),\n",
       " ('mobil', 0.9961954951286316),\n",
       " ('txt', 0.9959759712219238),\n",
       " ('valid', 0.9959015846252441),\n",
       " ('free', 0.9956607222557068)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('prize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guarante', 0.9986960887908936),\n",
       " ('claim', 0.9986200928688049),\n",
       " ('cash', 0.9973024725914001),\n",
       " ('won', 0.9966235160827637),\n",
       " ('call', 0.9964011311531067),\n",
       " ('award', 0.9962254166603088),\n",
       " ('mobil', 0.9961954951286316),\n",
       " ('txt', 0.9959759712219238),\n",
       " ('valid', 0.9959015846252441),\n",
       " ('free', 0.9956607222557068)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('prize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Default every word will have 100 dimension based on the algorithm irrespective of the size of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['kid'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02537386,  0.04020482,  0.03166584,  0.00650899,  0.02483618,\n",
       "       -0.18237102,  0.05610189,  0.192596  , -0.08526697, -0.05595316,\n",
       "       -0.0710955 , -0.1616685 , -0.06832775,  0.04895108,  0.02452326,\n",
       "       -0.07114151,  0.02100999, -0.08807635, -0.02583958, -0.23801003,\n",
       "        0.08337916,  0.0595209 ,  0.06847692, -0.0857102 , -0.04407695,\n",
       "       -0.02177232, -0.0964246 , -0.03618133, -0.05033445, -0.02138399,\n",
       "        0.12706777,  0.03418126,  0.02931222, -0.09569122, -0.02930906,\n",
       "        0.07957319, -0.00145616, -0.06761494, -0.02563707, -0.14360695,\n",
       "       -0.00593707, -0.10737401, -0.04574689, -0.00294601,  0.09299698,\n",
       "       -0.01344958, -0.08719249, -0.0125577 ,  0.03386301,  0.04250006,\n",
       "        0.04450629, -0.08727499, -0.02271529,  0.02027864, -0.06257565,\n",
       "        0.06349179,  0.07239536, -0.00720087, -0.09593702,  0.0771193 ,\n",
       "        0.02238348, -0.03031101, -0.00162736, -0.00869755, -0.09688896,\n",
       "        0.09872153,  0.03876401,  0.1217334 , -0.14175056,  0.13526165,\n",
       "       -0.08171286,  0.02268433,  0.1132341 , -0.02655641,  0.10043516,\n",
       "        0.04466954,  0.02499682, -0.04644895, -0.12727056,  0.00143314,\n",
       "       -0.07048313,  0.01939804, -0.1635453 ,  0.11189715, -0.00378512,\n",
       "       -0.02532479,  0.01885822,  0.09826347,  0.09205641,  0.02468614,\n",
       "        0.11438613,  0.06336138,  0.04305414,  0.01446047,  0.17893146,\n",
       "        0.12189932,  0.07891539, -0.05744266,  0.06111163,  0.00268056],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['kid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4 Average word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to calculate average as input would be a message which can have n no of word each represented by 100 dimension for us input dimension will be fixed that is 100 dimension so taking average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazi',\n",
       " 'avail',\n",
       " 'onli',\n",
       " 'in',\n",
       " 'bugi',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amor',\n",
       " 'wat']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0] # this is my first sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply average word2vec in entire sentences\n",
    "\n",
    "tqdm is a Python library for adding dynamic progress bars to loops and iterable processes, providing visual feedback on the progress of operations. It enhances the user experience when dealing with tasks that may take some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(doc):\n",
    "    return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1394/5570 [00:00<00:00, 4818.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 451\n",
      "['hank', 'lotsli']\n",
      "nan 783\n",
      "['beerag']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2567/5570 [00:00<00:00, 5463.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 1612\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 4810/5570 [00:00<00:00, 5407.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 4292\n",
      "[]\n",
      "nan 4479\n",
      "['erutupalam', 'thandiyachu']\n",
      "nan 5171\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5570/5570 [00:01<00:00, 4366.04it/s]\n"
     ]
    }
   ],
   "source": [
    "X_avg=[]\n",
    "y_filtered = []\n",
    "for i in tqdm(range(len(words))):\n",
    "    avg_word = avg_word2vec(words[i])\n",
    "    if not np.any(np.isnan(avg_word)):\n",
    "        X_avg.append(avg_word)\n",
    "        y_filtered.append(y[i])\n",
    "    else:       \n",
    "       print(avg_word,i)\n",
    "       print(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazi',\n",
       " 'avail',\n",
       " 'onli',\n",
       " 'in',\n",
       " 'bugi',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amor',\n",
       " 'wat']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first sentence as above we will get 100 dimension as below after average word2vec which will be input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04366148,  0.11668544,  0.07114939,  0.01470136,  0.04185769,\n",
       "       -0.5680695 ,  0.16200317,  0.63781804, -0.28876436, -0.14762105,\n",
       "       -0.24379629, -0.42528337, -0.23411198,  0.1165799 ,  0.02059597,\n",
       "       -0.20298767,  0.09633525, -0.26017863, -0.02152736, -0.76236796,\n",
       "        0.27895218,  0.21326505,  0.20262623, -0.26957974, -0.13466471,\n",
       "       -0.04359645, -0.28080478, -0.1077365 , -0.12348515,  0.00242672,\n",
       "        0.3634307 ,  0.09953444,  0.10516998, -0.27315974, -0.07501835,\n",
       "        0.26285535,  0.03683966, -0.24019669, -0.12278247, -0.4339307 ,\n",
       "       -0.0358152 , -0.33958405, -0.07211695,  0.03604535,  0.26791713,\n",
       "       -0.07560895, -0.29079822, -0.04241047,  0.13183649,  0.18005455,\n",
       "        0.09787506, -0.27541685, -0.05381407,  0.05990165, -0.18866974,\n",
       "        0.20182894,  0.1733481 , -0.04914084, -0.3553083 ,  0.19725378,\n",
       "        0.10818958, -0.07198407,  0.003163  ,  0.0194209 , -0.29225093,\n",
       "        0.30773202,  0.16519828,  0.37706205, -0.43621364,  0.4290453 ,\n",
       "       -0.25372362,  0.095723  ,  0.35565275, -0.13725774,  0.34738627,\n",
       "        0.1821707 ,  0.05173143, -0.10351981, -0.35785908,  0.00286273,\n",
       "       -0.22461455, -0.02338903, -0.48366436,  0.38787246, -0.03691018,\n",
       "       -0.05568976,  0.0268748 ,  0.28209916,  0.29993346,  0.02027518,\n",
       "        0.3674552 ,  0.2055844 ,  0.09040292,  0.07635003,  0.54799825,\n",
       "        0.3729255 ,  0.17568448, -0.19163759,  0.14980002,  0.01431583],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avg[0] #First sentence is having 100 dimension this is my input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_avg = [arr for arr in X_avg if not np.any(np.isnan(arr))]  # remove any more nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5564\n",
      "5564\n",
      "5570\n",
      "5570\n"
     ]
    }
   ],
   "source": [
    "print(len(X_avg))\n",
    "print(len(y_filtered))\n",
    "print(len(X))\n",
    "print(len(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_avg_train, X_avg_test, y_avg_train, y_avg_test = train_test_split(X_avg, y_filtered, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_avg_train,y_avg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred=classifier.predict(X_avg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9694519317160827"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_avg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.97      0.98       977\n",
      "        True       0.82      0.96      0.88       136\n",
      "\n",
      "    accuracy                           0.97      1113\n",
      "   macro avg       0.91      0.96      0.93      1113\n",
      "weighted avg       0.97      0.97      0.97      1113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred,y_avg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'you', 'have', 'won', 'lottery']\n"
     ]
    }
   ],
   "source": [
    "new_word = []\n",
    "new_email = \"Hey you have won a lottery\"\n",
    "sent_token = sent_tokenize(new_email)\n",
    "\n",
    "if sent_token:\n",
    "    for sentence_token in sent_token:\n",
    "        new_word.extend(simple_preprocess(sentence_token))  # extending the list instead of appending\n",
    "\n",
    "print(new_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create the feature and output based on window size and train the model to represent each word as vector dimension\n",
    "model1 =gensim.models.Word2Vec(new_word,window=5,min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x23fc9a84e90>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
