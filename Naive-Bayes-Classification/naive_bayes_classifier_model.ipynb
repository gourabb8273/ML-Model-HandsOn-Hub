{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of classifiers that are quite similar to the linear\n",
    "models discussed in the previous section. However, they tend to be even **faster** in\n",
    "training. \n",
    "\n",
    "The price paid for this efficiency is that naive Bayes models often provide\n",
    "generalization performance that is slightly **worse than that of linear classifiers** like\n",
    "LogisticRegression and LinearSVC.\n",
    "\n",
    "The reason that naive Bayes models are so efficient is that they learn parameters by\n",
    "looking at each feature individually and collect simple per-class statistics from each\n",
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three kinds of naive Bayes classifiers implemented in scikit68 \n",
    "1. GaussianNB, \n",
    "2. BernoulliNB, and\n",
    "3. MultinomialNB.\n",
    "\n",
    " GaussianNB can be applied to any continuous data,\n",
    " while BernoulliNB assumes binary data \n",
    " MultinomialNB assumes count data (that is, that each feature represents an integer count of some‐\n",
    "thing, like how often a word appears in a sentence).\n",
    "\n",
    "BernoulliNB and MultinomialNB are mostly used in text data classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BernoulliNB classifier counts how often every feature of each class is not zero.\n",
    "This is most easily understood with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 1, 0, 1],\n",
    " [1, 0, 1, 1],\n",
    " [0, 0, 0, 1],\n",
    " [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, X is a 4x4 NumPy array representing a dataset with four data points (rows) and four features (columns). y is an array representing the class labels (0 or 1) for each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For class 0 output in y (the first and third data points), the first feature (first column) is zero two times (0th and 2nd index)\n",
    "and nonzero zero times (as only for class 0 from y set), the second feature is zero one time and nonzero one time,\n",
    "and so on. These same counts are then calculated for the data points in the second\n",
    "class. \n",
    "\n",
    "Counting the nonzero entries per class in essence looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature counts:\n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    " # iterate over each class\n",
    " # count (sum) entries of 1 per feature\n",
    " counts[label] = X[y == label].sum(axis=0)\n",
    "print(\"Feature counts:\\n{}\".format(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The output Feature counts: {0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])} indicates the counts of nonzero entries for each feature, separated by class. Let's break it down:\n",
    "\n",
    "For Class 0:\n",
    "\n",
    "Feature 1 has 0 occurrences of 1 and 2 occurrences of 0.\n",
    "\n",
    "Feature 2 has 1 occurrence of 1 and 0 occurrences of 0.\n",
    "\n",
    "Feature 3 has 0 occurrences of 1 and 0 occurrences of 0.\n",
    "\n",
    "Feature 4 has 2 occurrences of 1 and 0 occurrences of 0.\n",
    "\n",
    "For Class 1:\n",
    "\n",
    "Feature 1 has 2 occurrences of 1 and 0 occurrences of 0.\n",
    "\n",
    "Feature 2 has 0 occurrences of 1 and 2 occurrences of 0.\n",
    "\n",
    "Feature 3 has 2 occurrences of 1 and 0 occurrences of 0.\n",
    "\n",
    "Feature 4 has 1 occurrence of 1 and 1 occurrence of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n",
    "ferent in what kinds of statistics they compute.\n",
    "\n",
    "***MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the average value as well as the standard deviation of each feature for each class.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction, a data point is compared to the statistics for each of the classes,\n",
    "and the best matching class is predicted. Interestingly, for both MultinomialNB and\n",
    "BernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\n",
    "ear models (see “Linear models for classification” on page 56). Unfortunately, coef_\n",
    "for the naive Bayes models has a somewhat different meaning than in the linear mod‐\n",
    "els, in that coef_ is not the same as w.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB and BernoulliNB have a single parameter, **alpha**, which controls\n",
    "model complexity. The way alpha works is that the algorithm adds to the data alpha\n",
    "many virtual data points that have positive values for all the features. This results in a\n",
    "“smoothing” of the statistics. \n",
    "\n",
    "***A large alpha means more smoothing, resulting in less complex models.***\n",
    " The algorithm’s performance is relatively robust to the setting of\n",
    "alpha, meaning that setting alpha is not critical for good performance. However,\n",
    "tuning it usually improves accuracy somewhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GaussianNB is mostly used on very high-dimensional data, while the other two variants of naive Bayes are widely used for sparse count data such as text.**\n",
    " MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively large\n",
    "number of nonzero features (i.e., large documents).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
